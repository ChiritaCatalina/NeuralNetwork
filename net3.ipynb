{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model de predictie multiparametru\n",
    "\n",
    "Modelul nostru citeste un fisier Excel si imparte datele de intrare in elemente de intrare si elemente de iesire. Elementele de intrare au 24 de caracteristici pe care le urmeaza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (918, 24), Y: (918,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(r'Copy of Extraction PT4 14x59 din 2016 pana in prezent - analysis S1928 (003).xlsx', sheet_name='TL4 -40 Nm' )\n",
    "dataset = df.values\n",
    "\n",
    "X = dataset[1:,23:47]\n",
    "Y = dataset[1:,14]\n",
    "X = X.astype('float')\n",
    "Y = Y.astype('float')\n",
    "print(\"X:  %s, Y: %s\" % (X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(911, 23) (911, 1)\n"
     ]
    }
   ],
   "source": [
    "Xdf = pd.DataFrame(X)\n",
    "Ydf = pd.DataFrame(Y)\n",
    "Xdf.drop(22, axis=1,inplace=True)\n",
    "Xdf[24] = Ydf[0]\n",
    "Ydf.plot()\n",
    "Xdf.dropna(inplace=True)\n",
    "td = Xdf.values\n",
    "X = td[0:, :-1]\n",
    "Y = td[0:, -1:]\n",
    "X = X.astype('float')\n",
    "Y = Y.astype('float')\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizarea datelor\n",
    "Este necesara extragerea din dataset a valorilor de intrare care au valoare NaN.\n",
    "De asemenea, am ignorat parametrii care nu varieaza in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((911, 23), (911, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalam datele de intrare si de iesire intre valorile 0 si 1\n",
    "pentru aceasta apelam la minmaxscaller din pachetul scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((911, 23), (911, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "Y_scale = Y_scaler.fit_transform(Y)\n",
    "\n",
    "X_scale.shape, Y_scale.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impartim datele in data de training si date de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(637, 23)\n",
      "(137, 23)\n",
      "(137, 23)\n",
      "(637, 1)\n",
      "(137, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y_scale, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cream modelul ML\n",
    "1 strat de intrare de dimensiunea dictata de numarul de parametrii folositi (22)\n",
    "\n",
    "1 strat ascuns dim 77 folosind modelul de activare ReLU\n",
    "\n",
    "1 strat ascuns dim 77 folosind modelul de activare ReLU\n",
    "\n",
    "1 strat de iesire dim 1 folosind un model de activare de tip sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0916 23:54:55.723056 11496 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0916 23:54:55.802073 11496 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0916 23:54:55.807007 11496 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0916 23:54:55.862063 11496 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0916 23:54:55.880067 11496 deprecation.py:506] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0916 23:54:56.875051 11496 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0916 23:54:58.816008 11496 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 637 samples, validate on 137 samples\n",
      "Epoch 1/100\n",
      "637/637 [==============================] - 6s 9ms/step - loss: 5.6896 - acc: 0.0016 - val_loss: 4.8789 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "637/637 [==============================] - ETA: 0s - loss: 4.4096 - acc: 0.0021    - 0s 392us/step - loss: 4.2590 - acc: 0.0016 - val_loss: 3.6220 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "637/637 [==============================] - 0s 350us/step - loss: 3.1485 - acc: 0.0016 - val_loss: 2.6654 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "637/637 [==============================] - 0s 359us/step - loss: 2.3093 - acc: 0.0016 - val_loss: 1.9472 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "637/637 [==============================] - 0s 356us/step - loss: 1.6821 - acc: 0.0016 - val_loss: 1.4123 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "637/637 [==============================] - 0s 356us/step - loss: 1.2165 - acc: 0.0016 - val_loss: 1.0172 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "637/637 [==============================] - 0s 353us/step - loss: 0.8739 - acc: 0.0016 - val_loss: 0.7281 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "637/637 [==============================] - 0s 374us/step - loss: 0.6245 - acc: 0.0016 - val_loss: 0.5189 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "637/637 [==============================] - 0s 370us/step - loss: 0.4449 - acc: 0.0016 - val_loss: 0.3694 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "637/637 [==============================] - 0s 372us/step - loss: 0.3172 - acc: 0.0016 - val_loss: 0.2637 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "637/637 [==============================] - 0s 380us/step - loss: 0.2276 - acc: 0.0016 - val_loss: 0.1901 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "637/637 [==============================] - 0s 361us/step - loss: 0.1655 - acc: 0.0016 - val_loss: 0.1394 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "637/637 [==============================] - 0s 364us/step - loss: 0.1231 - acc: 0.0016 - val_loss: 0.1051 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "637/637 [==============================] - 0s 360us/step - loss: 0.0945 - acc: 0.0016 - val_loss: 0.0821 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "637/637 [==============================] - 0s 356us/step - loss: 0.0755 - acc: 0.0016 - val_loss: 0.0669 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "637/637 [==============================] - 0s 341us/step - loss: 0.0629 - acc: 0.0016 - val_loss: 0.0570 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "637/637 [==============================] - 0s 395us/step - loss: 0.0548 - acc: 0.0016 - val_loss: 0.0506 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "637/637 [==============================] - 0s 362us/step - loss: 0.0497 - acc: 0.0016 - val_loss: 0.0467 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "637/637 [==============================] - 0s 353us/step - loss: 0.0464 - acc: 0.0016 - val_loss: 0.0441 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "637/637 [==============================] - 0s 370us/step - loss: 0.0444 - acc: 0.0016 - val_loss: 0.0425 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "637/637 [==============================] - 0s 364us/step - loss: 0.0431 - acc: 0.0016 - val_loss: 0.0416 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "637/637 [==============================] - 0s 349us/step - loss: 0.0424 - acc: 0.0016 - val_loss: 0.0410 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "637/637 [==============================] - 0s 352us/step - loss: 0.0419 - acc: 0.0016 - val_loss: 0.0406 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "637/637 [==============================] - 0s 356us/step - loss: 0.0416 - acc: 0.0016 - val_loss: 0.0404 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "637/637 [==============================] - 0s 348us/step - loss: 0.0415 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "637/637 [==============================] - 0s 359us/step - loss: 0.0413 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "637/637 [==============================] - 0s 368us/step - loss: 0.0413 - acc: 0.0016 - val_loss: 0.0401 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "637/637 [==============================] - 0s 388us/step - loss: 0.0412 - acc: 0.0016 - val_loss: 0.0401 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "637/637 [==============================] - 0s 407us/step - loss: 0.0412 - acc: 0.0016 - val_loss: 0.0400 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "637/637 [==============================] - 0s 402us/step - loss: 0.0412 - acc: 0.0016 - val_loss: 0.0400 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "637/637 [==============================] - 0s 404us/step - loss: 0.0412 - acc: 0.0016 - val_loss: 0.0400 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "637/637 [==============================] - 0s 396us/step - loss: 0.0412 - acc: 0.0016 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "637/637 [==============================] - 0s 385us/step - loss: 0.0411 - acc: 0.0016 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "637/637 [==============================] - 0s 353us/step - loss: 0.0412 - acc: 0.0016 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "637/637 [==============================] - 0s 363us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "637/637 [==============================] - 0s 345us/step - loss: 0.0411 - acc: 0.0016 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "637/637 [==============================] - 0s 336us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "637/637 [==============================] - 0s 342us/step - loss: 0.0411 - acc: 0.0016 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "637/637 [==============================] - 0s 341us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "637/637 [==============================] - 0s 316us/step - loss: 0.0411 - acc: 0.0016 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "637/637 [==============================] - 0s 347us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "637/637 [==============================] - 0s 355us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "637/637 [==============================] - 0s 325us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "637/637 [==============================] - 0s 349us/step - loss: 0.0411 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "637/637 [==============================] - 0s 333us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "637/637 [==============================] - 0s 322us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0399 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "637/637 [==============================] - 0s 309us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "637/637 [==============================] - 0s 325us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "637/637 [==============================] - 0s 330us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "637/637 [==============================] - 0s 330us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "637/637 [==============================] - 0s 317us/step - loss: 0.0411 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "637/637 [==============================] - 0s 319us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "637/637 [==============================] - 0s 375us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "637/637 [==============================] - 0s 349us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "637/637 [==============================] - 0s 369us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "637/637 [==============================] - 0s 348us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "637/637 [==============================] - 0s 359us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "637/637 [==============================] - 0s 345us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "637/637 [==============================] - 0s 358us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "637/637 [==============================] - 0s 370us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "637/637 [==============================] - 0s 370us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "637/637 [==============================] - 0s 364us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "637/637 [==============================] - 0s 376us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "637/637 [==============================] - 0s 355us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "637/637 [==============================] - 0s 372us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "637/637 [==============================] - 0s 363us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "637/637 [==============================] - 0s 363us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "637/637 [==============================] - 0s 358us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "637/637 [==============================] - 0s 366us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "637/637 [==============================] - 0s 358us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "637/637 [==============================] - 0s 334us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "637/637 [==============================] - 0s 324us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "637/637 [==============================] - 0s 319us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "637/637 [==============================] - 0s 319us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "637/637 [==============================] - 0s 312us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "637/637 [==============================] - 0s 342us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "637/637 [==============================] - 0s 345us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "637/637 [==============================] - 0s 334us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "637/637 [==============================] - 0s 347us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "637/637 [==============================] - 0s 339us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "637/637 [==============================] - 0s 322us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "637/637 [==============================] - 0s 339us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "637/637 [==============================] - 0s 369us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "637/637 [==============================] - 0s 350us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "637/637 [==============================] - 0s 353us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "637/637 [==============================] - 0s 345us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "637/637 [==============================] - 0s 344us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "637/637 [==============================] - 0s 342us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "637/637 [==============================] - 0s 328us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "637/637 [==============================] - 0s 312us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "637/637 [==============================] - 0s 333us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "637/637 [==============================] - 0s 375us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "637/637 [==============================] - 0s 350us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "637/637 [==============================] - 0s 340us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "637/637 [==============================] - 0s 341us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "637/637 [==============================] - 0s 344us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "637/637 [==============================] - 0s 360us/step - loss: 0.0410 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "637/637 [==============================] - ETA: 0s - loss: 0.0407 - acc: 0.0016  - 0s 381us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "637/637 [==============================] - 0s 421us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "637/637 [==============================] - 0s 394us/step - loss: 0.0409 - acc: 0.0016 - val_loss: 0.0397 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "model = Sequential([\n",
    "    Dense(32, activation='tanh', kernel_initializer='uniform',kernel_regularizer=regularizers.l2(0.01), input_shape=(23,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))\n",
    "])\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='hinge')\n",
    "#model.compile(optimizer='rmsprop', loss='hinge')\n",
    "\n",
    "#model.compile(optimizer='Adadelta', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "#model.compile(optimizer='sgd', loss='hinge', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "#, metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='mean_squared_logarithmic_error')\n",
    "# model.compile(optimizer='rmsprop', loss=\"mean_absolute_percentage_error\")\n",
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=100,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57518584]] [[5.55]] [[9.928532]]\n",
      "[[0.57518584]] [[7.26]] [[9.928532]]\n",
      "[[0.57518584]] [[3.48]] [[9.928532]]\n",
      "[[0.57518584]] [[4.14]] [[9.928532]]\n",
      "[[0.57518584]] [[6.99]] [[9.928532]]\n",
      "[[0.57518584]] [[8.84]] [[9.928532]]\n",
      "[[0.57518584]] [[4.41]] [[9.928532]]\n",
      "[[0.57518584]] [[7.89]] [[9.928532]]\n",
      "[[0.57518584]] [[7.57]] [[9.928532]]\n",
      "[[0.57518584]] [[7.49]] [[9.928532]]\n",
      "[[0.57518584]] [[11.36]] [[9.928532]]\n",
      "[[0.57518584]] [[12.12]] [[9.928532]]\n",
      "[[0.57518584]] [[10.3]] [[9.928532]]\n",
      "[[0.57518584]] [[2.]] [[9.928532]]\n",
      "[[0.57518584]] [[12.52]] [[9.928532]]\n",
      "[[0.57518584]] [[10.95]] [[9.928532]]\n",
      "[[0.57518584]] [[11.52]] [[9.928532]]\n",
      "[[0.57518584]] [[12.45]] [[9.928532]]\n",
      "[[0.57518584]] [[11.]] [[9.928532]]\n",
      "[[0.57518584]] [[13.13]] [[9.928532]]\n",
      "[[0.57518584]] [[7.51]] [[9.928532]]\n",
      "[[0.57518584]] [[15.8]] [[9.928532]]\n",
      "[[0.57518584]] [[13.71]] [[9.928532]]\n",
      "[[0.57518584]] [[13.43]] [[9.928532]]\n",
      "[[0.57518584]] [[15.]] [[9.928532]]\n",
      "[[0.57518584]] [[14.]] [[9.928532]]\n",
      "[[0.57518584]] [[4.]] [[9.928532]]\n",
      "[[0.57518584]] [[11.79]] [[9.928532]]\n",
      "[[0.57518584]] [[4.08]] [[9.928532]]\n",
      "[[0.57518584]] [[9.]] [[9.928532]]\n",
      "[[0.57518584]] [[9.71]] [[9.928532]]\n",
      "[[0.57518584]] [[12.22]] [[9.928532]]\n",
      "[[0.57518584]] [[4.]] [[9.928532]]\n",
      "[[0.57518584]] [[13.]] [[9.928532]]\n",
      "[[0.57518584]] [[9.57]] [[9.928532]]\n",
      "[[0.57518584]] [[11.79]] [[9.928532]]\n",
      "[[0.57518584]] [[8.6]] [[9.928532]]\n",
      "[[0.57518584]] [[5.79]] [[9.928532]]\n",
      "[[0.57518584]] [[8.55]] [[9.928532]]\n",
      "[[0.57518584]] [[13.]] [[9.928532]]\n",
      "[[0.57518584]] [[6.62]] [[9.928532]]\n",
      "[[0.57518584]] [[5.66]] [[9.928532]]\n",
      "[[0.57518584]] [[9.21]] [[9.928532]]\n",
      "[[0.57518584]] [[13.91]] [[9.928532]]\n",
      "[[0.57518584]] [[2.]] [[9.928532]]\n",
      "[[0.57518584]] [[11.69]] [[9.928532]]\n",
      "[[0.57518584]] [[12.06]] [[9.928532]]\n",
      "[[0.57518584]] [[9.5]] [[9.928532]]\n",
      "[[0.57518584]] [[12.76]] [[9.928532]]\n",
      "[[0.57518584]] [[10.67]] [[9.928532]]\n",
      "[[0.57518584]] [[7.67]] [[9.928532]]\n",
      "[[0.57518584]] [[9.4]] [[9.928532]]\n",
      "[[0.57518584]] [[8.6]] [[9.928532]]\n",
      "[[0.57518584]] [[-1.]] [[9.928532]]\n",
      "[[0.57518584]] [[5.31]] [[9.928532]]\n",
      "[[0.57518584]] [[15.76]] [[9.928532]]\n",
      "[[0.57518584]] [[9.34]] [[9.928532]]\n",
      "[[0.57518584]] [[10.02]] [[9.928532]]\n",
      "[[0.57518584]] [[15.02]] [[9.928532]]\n",
      "[[0.57518584]] [[14.]] [[9.928532]]\n",
      "[[0.57518584]] [[10.]] [[9.928532]]\n",
      "[[0.57518584]] [[14.]] [[9.928532]]\n",
      "[[0.57518584]] [[3.23]] [[9.928532]]\n",
      "[[0.57518584]] [[2.]] [[9.928532]]\n",
      "[[0.57518584]] [[9.]] [[9.928532]]\n",
      "[[0.57518584]] [[7.47]] [[9.928532]]\n",
      "[[0.57518584]] [[10.69]] [[9.928532]]\n",
      "[[0.57518584]] [[13.05]] [[9.928532]]\n",
      "[[0.57518584]] [[7.84]] [[9.928532]]\n",
      "[[0.57518584]] [[4.19]] [[9.928532]]\n",
      "[[0.57518584]] [[5.65]] [[9.928532]]\n",
      "[[0.57518584]] [[4.51]] [[9.928532]]\n",
      "[[0.57518584]] [[8.29]] [[9.928532]]\n",
      "[[0.57518584]] [[13.48]] [[9.928532]]\n",
      "[[0.57518584]] [[5.65]] [[9.928532]]\n",
      "[[0.57518584]] [[8.]] [[9.928532]]\n",
      "[[0.57518584]] [[15.]] [[9.928532]]\n",
      "[[0.57518584]] [[7.]] [[9.928532]]\n",
      "[[0.57518584]] [[8.58]] [[9.928532]]\n",
      "[[0.57518584]] [[13.21]] [[9.928532]]\n",
      "[[0.57518584]] [[10.98]] [[9.928532]]\n",
      "[[0.57518584]] [[8.13]] [[9.928532]]\n",
      "[[0.57518584]] [[8.54]] [[9.928532]]\n",
      "[[0.57518584]] [[14.]] [[9.928532]]\n",
      "[[0.57518584]] [[11.6]] [[9.928532]]\n",
      "[[0.57518584]] [[3.]] [[9.928532]]\n",
      "[[0.57518584]] [[8.]] [[9.928532]]\n",
      "[[0.57518584]] [[10.]] [[9.928532]]\n",
      "[[0.57518584]] [[7.6]] [[9.928532]]\n",
      "[[0.57518584]] [[10.]] [[9.928532]]\n",
      "[[0.57518584]] [[6.67]] [[9.928532]]\n",
      "[[0.57518584]] [[11.72]] [[9.928532]]\n",
      "[[0.57518584]] [[10.96]] [[9.928532]]\n",
      "[[0.57518584]] [[0.]] [[9.928532]]\n",
      "[[0.57518584]] [[9.23]] [[9.928532]]\n",
      "[[0.57518584]] [[9.61]] [[9.928532]]\n",
      "[[0.57518584]] [[8.]] [[9.928532]]\n",
      "[[0.57518584]] [[5.73]] [[9.928532]]\n",
      "[[0.57518584]] [[13.63]] [[9.928532]]\n",
      "[[0.57518584]] [[8.14]] [[9.928532]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(100):\n",
    "#    pass\n",
    "#    p = model.predict_on_batch(X_test[i].reshape(1, 23))\n",
    "    p = model.predict(X_test[i].reshape(1,23))\n",
    "    print(p,Y_scaler.inverse_transform(np.array(Y_test[i]).reshape(-1,1)), Y_scaler.inverse_transform(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637/637 [==============================] - 0s 83us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.040898688634626534, 0.0015698587127158557]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.57518584]], dtype=float32),\n",
       " array([0.94473684]),\n",
       " array([[9.928532]], dtype=float32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0].reshape(1, 23)),Y_train[0], Y_scaler.inverse_transform(model.predict(X_train[0].reshape(1, 23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 124us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03970077081862157, 0.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, Y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
