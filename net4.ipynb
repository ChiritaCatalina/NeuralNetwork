{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model de predictie multiparametru\n",
    "\n",
    "Modelul nostru citeste un fisier Excel si imparte datele de intrare in elemente de intrare si elemente de iesire. Elementele de intrare au 24 de caracteristici pe care le urmeaza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (918, 24), Y: (918,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(r'Copy of Extraction PT4 14x59 din 2016 pana in prezent - analysis S1928 (003).xlsx', sheet_name='TL4 -40 Nm' )\n",
    "dataset = df.values\n",
    "\n",
    "X = dataset[1:,23:47]\n",
    "Y = dataset[1:,14]\n",
    "X = X.astype('float')\n",
    "Y = Y.astype('float')\n",
    "print(\"X:  %s, Y: %s\" % (X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(911, 23) (911, 1)\n"
     ]
    }
   ],
   "source": [
    "Xdf = pd.DataFrame(X)\n",
    "Ydf = pd.DataFrame(Y)\n",
    "Xdf.drop(22, axis=1,inplace=True)\n",
    "Xdf[24] = Ydf[0]\n",
    "Ydf.plot()\n",
    "Xdf.dropna(inplace=True)\n",
    "td = Xdf.values\n",
    "X = td[0:, :-1]\n",
    "Y = td[0:, -1:]\n",
    "X = X.astype('float')\n",
    "Y = Y.astype('float')\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizarea datelor\n",
    "Este necesara extragerea din dataset a valorilor de intrare care au valoare NaN.\n",
    "De asemenea, am ignorat parametrii care nu varieaza in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((911, 23), (911, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalam datele de intrare si de iesire intre valorile 0 si 1\n",
    "pentru aceasta apelam la minmaxscaller din pachetul scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((911, 23), (911, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "Y_scale = Y_scaler.fit_transform(Y)\n",
    "\n",
    "X_scale.shape, Y_scale.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impartim datele in data de training si date de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(637, 23)\n",
      "(137, 23)\n",
      "(137, 23)\n",
      "(637, 1)\n",
      "(137, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y_scale, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cream modelul ML\n",
    "1 strat de intrare de dimensiunea dictata de numarul de parametrii folositi (22)\n",
    "\n",
    "1 strat ascuns dim 77 folosind modelul de activare ReLU\n",
    "\n",
    "1 strat ascuns dim 77 folosind modelul de activare ReLU\n",
    "\n",
    "1 strat de iesire dim 1 folosind un model de activare de tip sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 00:01:28.034684 15264 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0917 00:01:28.102732 15264 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0917 00:01:28.109161 15264 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0917 00:01:28.181703 15264 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0917 00:01:28.212701 15264 deprecation.py:506] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0917 00:01:29.260729 15264 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0917 00:01:31.095693 15264 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 637 samples, validate on 137 samples\n",
      "Epoch 1/100\n",
      "637/637 [==============================] - 6s 9ms/step - loss: 3.0211 - acc: 0.0016 - val_loss: 2.7188 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "637/637 [==============================] - 0s 254us/step - loss: 2.4786 - acc: 0.0016 - val_loss: 2.2197 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "637/637 [==============================] - 0s 281us/step - loss: 2.0189 - acc: 0.0016 - val_loss: 1.8048 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "637/637 [==============================] - 0s 276us/step - loss: 1.6397 - acc: 0.0016 - val_loss: 1.4637 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "637/637 [==============================] - 0s 297us/step - loss: 1.3282 - acc: 0.0016 - val_loss: 1.1836 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "637/637 [==============================] - 0s 279us/step - loss: 1.0736 - acc: 0.0016 - val_loss: 0.9551 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "637/637 [==============================] - 0s 278us/step - loss: 0.8656 - acc: 0.0016 - val_loss: 0.7688 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "637/637 [==============================] - 0s 276us/step - loss: 0.6966 - acc: 0.0016 - val_loss: 0.6175 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "637/637 [==============================] - 0s 293us/step - loss: 0.5592 - acc: 0.0016 - val_loss: 0.4951 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "637/637 [==============================] - 0s 294us/step - loss: 0.4483 - acc: 0.0016 - val_loss: 0.3967 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "637/637 [==============================] - 0s 287us/step - loss: 0.3597 - acc: 0.0016 - val_loss: 0.3180 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "637/637 [==============================] - 0s 195us/step - loss: 0.2890 - acc: 0.0016 - val_loss: 0.2554 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "637/637 [==============================] - 0s 229us/step - loss: 0.2329 - acc: 0.0016 - val_loss: 0.2060 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "637/637 [==============================] - 0s 248us/step - loss: 0.1887 - acc: 0.0016 - val_loss: 0.1671 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "637/637 [==============================] - 0s 301us/step - loss: 0.1539 - acc: 0.0016 - val_loss: 0.1368 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "637/637 [==============================] - 0s 287us/step - loss: 0.1269 - acc: 0.0016 - val_loss: 0.1133 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "637/637 [==============================] - 0s 270us/step - loss: 0.1060 - acc: 0.0016 - val_loss: 0.0952 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "637/637 [==============================] - 0s 281us/step - loss: 0.0902 - acc: 0.0016 - val_loss: 0.0813 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "637/637 [==============================] - 0s 285us/step - loss: 0.0776 - acc: 0.0016 - val_loss: 0.0708 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "637/637 [==============================] - 0s 270us/step - loss: 0.0686 - acc: 0.0016 - val_loss: 0.0628 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "637/637 [==============================] - 0s 279us/step - loss: 0.0615 - acc: 0.0016 - val_loss: 0.0568 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "637/637 [==============================] - 0s 284us/step - loss: 0.0564 - acc: 0.0016 - val_loss: 0.0524 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "637/637 [==============================] - 0s 275us/step - loss: 0.0524 - acc: 0.0016 - val_loss: 0.0491 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "637/637 [==============================] - 0s 298us/step - loss: 0.0497 - acc: 0.0016 - val_loss: 0.0467 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "637/637 [==============================] - 0s 286us/step - loss: 0.0475 - acc: 0.0016 - val_loss: 0.0449 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "637/637 [==============================] - 0s 243us/step - loss: 0.0460 - acc: 0.0016 - val_loss: 0.0436 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "637/637 [==============================] - 0s 240us/step - loss: 0.0449 - acc: 0.0016 - val_loss: 0.0427 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "637/637 [==============================] - 0s 265us/step - loss: 0.0441 - acc: 0.0016 - val_loss: 0.0420 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "637/637 [==============================] - 0s 270us/step - loss: 0.0435 - acc: 0.0016 - val_loss: 0.0415 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "637/637 [==============================] - 0s 259us/step - loss: 0.0430 - acc: 0.0016 - val_loss: 0.0412 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "637/637 [==============================] - ETA: 0s - loss: 0.0397 - acc: 0.0000e+0 - 0s 262us/step - loss: 0.0427 - acc: 0.0016 - val_loss: 0.0409 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "637/637 [==============================] - 0s 303us/step - loss: 0.0425 - acc: 0.0016 - val_loss: 0.0407 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "637/637 [==============================] - 0s 321us/step - loss: 0.0423 - acc: 0.0016 - val_loss: 0.0406 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "637/637 [==============================] - 0s 239us/step - loss: 0.0422 - acc: 0.0016 - val_loss: 0.0405 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "637/637 [==============================] - 0s 292us/step - loss: 0.0422 - acc: 0.0016 - val_loss: 0.0404 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "637/637 [==============================] - 0s 264us/step - loss: 0.0420 - acc: 0.0016 - val_loss: 0.0404 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "637/637 [==============================] - 0s 256us/step - loss: 0.0420 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "637/637 [==============================] - 0s 268us/step - loss: 0.0419 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "637/637 [==============================] - 0s 224us/step - loss: 0.0420 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "637/637 [==============================] - 0s 217us/step - loss: 0.0419 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      "637/637 [==============================] - 0s 265us/step - loss: 0.0419 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 42/100\n",
      "637/637 [==============================] - 0s 265us/step - loss: 0.0419 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 43/100\n",
      "637/637 [==============================] - 0s 272us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 44/100\n",
      "637/637 [==============================] - 0s 221us/step - loss: 0.0419 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 45/100\n",
      "637/637 [==============================] - 0s 220us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 46/100\n",
      "637/637 [==============================] - 0s 276us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 47/100\n",
      "637/637 [==============================] - 0s 237us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 48/100\n",
      "637/637 [==============================] - 0s 309us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 49/100\n",
      "637/637 [==============================] - 0s 206us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 50/100\n",
      "637/637 [==============================] - 0s 204us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 51/100\n",
      "637/637 [==============================] - 0s 188us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 52/100\n",
      "637/637 [==============================] - 0s 192us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 53/100\n",
      "637/637 [==============================] - 0s 213us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 54/100\n",
      "637/637 [==============================] - 0s 225us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 55/100\n",
      "637/637 [==============================] - 0s 265us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 56/100\n",
      "637/637 [==============================] - 0s 298us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 57/100\n",
      "637/637 [==============================] - 0s 248us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 58/100\n",
      "637/637 [==============================] - 0s 248us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "637/637 [==============================] - 0s 270us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 60/100\n",
      "637/637 [==============================] - 0s 272us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 61/100\n",
      "637/637 [==============================] - 0s 257us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 62/100\n",
      "637/637 [==============================] - 0s 262us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 63/100\n",
      "637/637 [==============================] - 0s 240us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 64/100\n",
      "637/637 [==============================] - 0s 290us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 65/100\n",
      "637/637 [==============================] - 0s 268us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 66/100\n",
      "637/637 [==============================] - 0s 268us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 67/100\n",
      "637/637 [==============================] - 0s 248us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 68/100\n",
      "637/637 [==============================] - 0s 248us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 69/100\n",
      "637/637 [==============================] - 0s 295us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 70/100\n",
      "637/637 [==============================] - 0s 266us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 71/100\n",
      "637/637 [==============================] - 0s 257us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 72/100\n",
      "637/637 [==============================] - 0s 244us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 73/100\n",
      "637/637 [==============================] - 0s 295us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 74/100\n",
      "637/637 [==============================] - 0s 277us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 75/100\n",
      "637/637 [==============================] - 0s 267us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 76/100\n",
      "637/637 [==============================] - 0s 264us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 77/100\n",
      "637/637 [==============================] - 0s 250us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 78/100\n",
      "637/637 [==============================] - 0s 248us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 79/100\n",
      "637/637 [==============================] - 0s 297us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 80/100\n",
      "637/637 [==============================] - 0s 279us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 81/100\n",
      "637/637 [==============================] - 0s 284us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 82/100\n",
      "637/637 [==============================] - 0s 226us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 83/100\n",
      "637/637 [==============================] - 0s 305us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 84/100\n",
      "637/637 [==============================] - 0s 285us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 85/100\n",
      "637/637 [==============================] - 0s 214us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 86/100\n",
      "637/637 [==============================] - 0s 232us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 87/100\n",
      "637/637 [==============================] - 0s 212us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 88/100\n",
      "637/637 [==============================] - 0s 239us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 89/100\n",
      "637/637 [==============================] - 0s 217us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 90/100\n",
      "637/637 [==============================] - 0s 217us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 91/100\n",
      "637/637 [==============================] - 0s 274us/step - loss: 0.0418 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 92/100\n",
      "637/637 [==============================] - 0s 287us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 93/100\n",
      "637/637 [==============================] - 0s 237us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 94/100\n",
      "637/637 [==============================] - ETA: 0s - loss: 0.0400 - acc: 0.0000e+0 - 0s 284us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 95/100\n",
      "637/637 [==============================] - 0s 259us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 96/100\n",
      "637/637 [==============================] - 0s 200us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 97/100\n",
      "637/637 [==============================] - 0s 187us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0403 - val_acc: 0.0000e+00\n",
      "Epoch 98/100\n",
      "637/637 [==============================] - 0s 270us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 99/100\n",
      "637/637 [==============================] - 0s 308us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n",
      "Epoch 100/100\n",
      "637/637 [==============================] - 0s 292us/step - loss: 0.0417 - acc: 0.0016 - val_loss: 0.0402 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "model = Sequential([\n",
    "    Dense(32, activation='tanh', kernel_initializer='uniform',kernel_regularizer=regularizers.l2(0.01), input_shape=(23,)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))\n",
    "])\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='hinge')\n",
    "#model.compile(optimizer='rmsprop', loss='hinge')\n",
    "\n",
    "#model.compile(optimizer='Adadelta', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "#model.compile(optimizer='sgd', loss='hinge', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "#, metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='mean_squared_logarithmic_error')\n",
    "# model.compile(optimizer='rmsprop', loss=\"mean_absolute_percentage_error\")\n",
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=100,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5702995]] [[13.84]] [[9.835691]]\n",
      "[[0.5702995]] [[4.]] [[9.835691]]\n",
      "[[0.5702995]] [[9.69]] [[9.835691]]\n",
      "[[0.5702995]] [[4.]] [[9.835691]]\n",
      "[[0.5702995]] [[11.77]] [[9.835691]]\n",
      "[[0.5702995]] [[11.69]] [[9.835691]]\n",
      "[[0.5702995]] [[15.]] [[9.835691]]\n",
      "[[0.5702995]] [[2.]] [[9.835691]]\n",
      "[[0.5702995]] [[12.01]] [[9.835691]]\n",
      "[[0.5702995]] [[11.64]] [[9.835691]]\n",
      "[[0.5702995]] [[7.89]] [[9.835691]]\n",
      "[[0.5702995]] [[14.9]] [[9.835691]]\n",
      "[[0.5702995]] [[9.69]] [[9.835691]]\n",
      "[[0.5702995]] [[11.]] [[9.835691]]\n",
      "[[0.5702995]] [[6.]] [[9.835691]]\n",
      "[[0.5702995]] [[8.]] [[9.835691]]\n",
      "[[0.5702995]] [[15.17]] [[9.835691]]\n",
      "[[0.5702995]] [[7.57]] [[9.835691]]\n",
      "[[0.5702995]] [[13.16]] [[9.835691]]\n",
      "[[0.5702995]] [[15.6]] [[9.835691]]\n",
      "[[0.5702995]] [[6.13]] [[9.835691]]\n",
      "[[0.5702995]] [[11.]] [[9.835691]]\n",
      "[[0.5702995]] [[9.]] [[9.835691]]\n",
      "[[0.5702995]] [[13.31]] [[9.835691]]\n",
      "[[0.5702995]] [[11.6]] [[9.835691]]\n",
      "[[0.5702995]] [[7.43]] [[9.835691]]\n",
      "[[0.5702995]] [[13.46]] [[9.835691]]\n",
      "[[0.5702995]] [[13.27]] [[9.835691]]\n",
      "[[0.5702995]] [[7.26]] [[9.835691]]\n",
      "[[0.5702995]] [[8.2]] [[9.835691]]\n",
      "[[0.5702995]] [[5.66]] [[9.835691]]\n",
      "[[0.5702995]] [[14.34]] [[9.835691]]\n",
      "[[0.5702995]] [[12.12]] [[9.835691]]\n",
      "[[0.5702995]] [[13.]] [[9.835691]]\n",
      "[[0.5702995]] [[8.26]] [[9.835691]]\n",
      "[[0.5702995]] [[9.98]] [[9.835691]]\n",
      "[[0.5702995]] [[10.43]] [[9.835691]]\n",
      "[[0.5702995]] [[13.]] [[9.835691]]\n",
      "[[0.5702995]] [[12.]] [[9.835691]]\n",
      "[[0.5702995]] [[6.99]] [[9.835691]]\n",
      "[[0.5702995]] [[12.31]] [[9.835691]]\n",
      "[[0.5702995]] [[12.]] [[9.835691]]\n",
      "[[0.5702995]] [[4.]] [[9.835691]]\n",
      "[[0.5702995]] [[13.]] [[9.835691]]\n",
      "[[0.5702995]] [[12.]] [[9.835691]]\n",
      "[[0.5702995]] [[13.85]] [[9.835691]]\n",
      "[[0.5702995]] [[15.]] [[9.835691]]\n",
      "[[0.5702995]] [[15.92]] [[9.835691]]\n",
      "[[0.5702995]] [[11.58]] [[9.835691]]\n",
      "[[0.5702995]] [[9.1]] [[9.835691]]\n",
      "[[0.5702995]] [[11.02]] [[9.835691]]\n",
      "[[0.5702995]] [[11.]] [[9.835691]]\n",
      "[[0.5702995]] [[6.13]] [[9.835691]]\n",
      "[[0.5702995]] [[8.]] [[9.835691]]\n",
      "[[0.5702995]] [[2.87]] [[9.835691]]\n",
      "[[0.5702995]] [[5.24]] [[9.835691]]\n",
      "[[0.5702995]] [[10.86]] [[9.835691]]\n",
      "[[0.5702995]] [[13.91]] [[9.835691]]\n",
      "[[0.5702995]] [[7.]] [[9.835691]]\n",
      "[[0.5702995]] [[2.37]] [[9.835691]]\n",
      "[[0.5702995]] [[10.]] [[9.835691]]\n",
      "[[0.5702995]] [[9.]] [[9.835691]]\n",
      "[[0.5702995]] [[2.]] [[9.835691]]\n",
      "[[0.5702995]] [[6.43]] [[9.835691]]\n",
      "[[0.5702995]] [[10.]] [[9.835691]]\n",
      "[[0.5702995]] [[6.22]] [[9.835691]]\n",
      "[[0.5702995]] [[9.86]] [[9.835691]]\n",
      "[[0.5702995]] [[10.43]] [[9.835691]]\n",
      "[[0.5702995]] [[13.84]] [[9.835691]]\n",
      "[[0.5702995]] [[9.18]] [[9.835691]]\n",
      "[[0.5702995]] [[14.]] [[9.835691]]\n",
      "[[0.5702995]] [[14.58]] [[9.835691]]\n",
      "[[0.5702995]] [[13.71]] [[9.835691]]\n",
      "[[0.5702995]] [[13.27]] [[9.835691]]\n",
      "[[0.5702995]] [[3.96]] [[9.835691]]\n",
      "[[0.5702995]] [[9.79]] [[9.835691]]\n",
      "[[0.5702995]] [[14.27]] [[9.835691]]\n",
      "[[0.5702995]] [[16.83]] [[9.835691]]\n",
      "[[0.5702995]] [[10.62]] [[9.835691]]\n",
      "[[0.5702995]] [[7.27]] [[9.835691]]\n",
      "[[0.5702995]] [[8.]] [[9.835691]]\n",
      "[[0.5702995]] [[13.]] [[9.835691]]\n",
      "[[0.5702995]] [[8.88]] [[9.835691]]\n",
      "[[0.5702995]] [[13.07]] [[9.835691]]\n",
      "[[0.5702995]] [[5.01]] [[9.835691]]\n",
      "[[0.5702995]] [[2.59]] [[9.835691]]\n",
      "[[0.5702995]] [[13.]] [[9.835691]]\n",
      "[[0.5702995]] [[6.64]] [[9.835691]]\n",
      "[[0.5702995]] [[9.03]] [[9.835691]]\n",
      "[[0.5702995]] [[6.]] [[9.835691]]\n",
      "[[0.5702995]] [[9.83]] [[9.835691]]\n",
      "[[0.5702995]] [[7.]] [[9.835691]]\n",
      "[[0.5702995]] [[9.]] [[9.835691]]\n",
      "[[0.5702995]] [[7.49]] [[9.835691]]\n",
      "[[0.5702995]] [[5.66]] [[9.835691]]\n",
      "[[0.5702995]] [[11.]] [[9.835691]]\n",
      "[[0.5702995]] [[9.57]] [[9.835691]]\n",
      "[[0.5702995]] [[12.76]] [[9.835691]]\n",
      "[[0.5702995]] [[14.41]] [[9.835691]]\n",
      "[[0.5702995]] [[15.46]] [[9.835691]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(100):\n",
    "#    pass\n",
    "#    p = model.predict_on_batch(X_test[i].reshape(1, 23))\n",
    "    p = model.predict(X_test[i].reshape(1,23))\n",
    "    print(p,Y_scaler.inverse_transform(np.array(Y_test[i]).reshape(-1,1)), Y_scaler.inverse_transform(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637/637 [==============================] - 0s 71us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04172180291247705, 0.0015698587127158557]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.5702995]], dtype=float32),\n",
       " array([0.33368421]),\n",
       " array([[9.835691]], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0].reshape(1, 23)),Y_train[0], Y_scaler.inverse_transform(model.predict(X_train[0].reshape(1, 23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 84us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.040199259255271756, 0.0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                768       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 11,361\n",
      "Trainable params: 11,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
