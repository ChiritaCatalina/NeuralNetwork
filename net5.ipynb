{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model de predictie multiparametru\n",
    "\n",
    "Modelul nostru citeste un fisier Excel si imparte datele de intrare in elemente de intrare si elemente de iesire. Elementele de intrare au 24 de caracteristici pe care le urmeaza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  (918, 24), Y: (918,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(r'Copy of Extraction PT4 14x59 din 2016 pana in prezent - analysis S1928 (003).xlsx', sheet_name='TL4 -40 Nm' )\n",
    "dataset = df.values\n",
    "\n",
    "X = dataset[1:,23:47]\n",
    "Y = dataset[1:,14]\n",
    "X = X.astype('float')\n",
    "Y = Y.astype('float')\n",
    "print(\"X:  %s, Y: %s\" % (X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(911, 23) (911, 1)\n"
     ]
    }
   ],
   "source": [
    "Xdf = pd.DataFrame(X)\n",
    "Ydf = pd.DataFrame(Y)\n",
    "Xdf.drop(22, axis=1,inplace=True)\n",
    "Xdf[24] = Ydf[0]\n",
    "Ydf.plot()\n",
    "Xdf.dropna(inplace=True)\n",
    "td = Xdf.values\n",
    "X = td[0:, :-1]\n",
    "Y = td[0:, -1:]\n",
    "X = X.astype('float')\n",
    "Y = Y.astype('float')\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizarea datelor\n",
    "Este necesara extragerea din dataset a valorilor de intrare care au valoare NaN.\n",
    "De asemenea, am ignorat parametrii care nu varieaza in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((911, 23), (911, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalam datele de intrare si de iesire intre valorile 0 si 1\n",
    "pentru aceasta apelam la minmaxscaller din pachetul scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((911, 23), (911, 1))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_scale = min_max_scaler.fit_transform(X)\n",
    "Y_scaler = preprocessing.MinMaxScaler()\n",
    "Y_scale = Y_scaler.fit_transform(Y)\n",
    "\n",
    "X_scale.shape, Y_scale.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impartim datele in data de training si date de test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(637, 23)\n",
      "(137, 23)\n",
      "(137, 23)\n",
      "(637, 1)\n",
      "(137, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y_scale, test_size=0.3)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cream modelul ML\n",
    "1 strat de intrare de dimensiunea dictata de numarul de parametrii folositi (22)\n",
    "\n",
    "1 strat ascuns dim 77 folosind modelul de activare ReLU\n",
    "\n",
    "1 strat ascuns dim 77 folosind modelul de activare ReLU\n",
    "\n",
    "1 strat de iesire dim 1 folosind un model de activare de tip sigmoid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 09:16:30.586233  7292 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0917 09:16:30.614234  7292 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0917 09:16:30.617232  7292 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0917 09:16:30.636236  7292 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0917 09:16:30.644232  7292 deprecation.py:506] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0917 09:16:30.751233  7292 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0917 09:16:31.023229  7292 deprecation_wrapper.py:119] From C:\\Users\\Alecs\\Anaconda3\\envs\\CChirita\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 637 samples, validate on 137 samples\n",
      "Epoch 1/100\n",
      "637/637 [==============================] - 1s 2ms/step - loss: 0.6069 - mean_absolute_error: 0.1747 - val_loss: 0.3496 - val_mean_absolute_error: 0.1544\n",
      "Epoch 2/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.2536 - mean_absolute_error: 0.1709 - val_loss: 0.1730 - val_mean_absolute_error: 0.1556\n",
      "Epoch 3/100\n",
      "637/637 [==============================] - 0s 119us/step - loss: 0.1567 - mean_absolute_error: 0.1713 - val_loss: 0.1330 - val_mean_absolute_error: 0.1556\n",
      "Epoch 4/100\n",
      "637/637 [==============================] - 0s 144us/step - loss: 0.1335 - mean_absolute_error: 0.1707 - val_loss: 0.1197 - val_mean_absolute_error: 0.1547\n",
      "Epoch 5/100\n",
      "637/637 [==============================] - 0s 151us/step - loss: 0.1226 - mean_absolute_error: 0.1701 - val_loss: 0.1103 - val_mean_absolute_error: 0.1547\n",
      "Epoch 6/100\n",
      "637/637 [==============================] - 0s 159us/step - loss: 0.1138 - mean_absolute_error: 0.1699 - val_loss: 0.1020 - val_mean_absolute_error: 0.1544\n",
      "Epoch 7/100\n",
      "637/637 [==============================] - 0s 140us/step - loss: 0.1059 - mean_absolute_error: 0.1700 - val_loss: 0.0946 - val_mean_absolute_error: 0.1541\n",
      "Epoch 8/100\n",
      "637/637 [==============================] - 0s 135us/step - loss: 0.0989 - mean_absolute_error: 0.1693 - val_loss: 0.0880 - val_mean_absolute_error: 0.1540\n",
      "Epoch 9/100\n",
      "637/637 [==============================] - 0s 137us/step - loss: 0.0928 - mean_absolute_error: 0.1695 - val_loss: 0.0821 - val_mean_absolute_error: 0.1538\n",
      "Epoch 10/100\n",
      "637/637 [==============================] - 0s 122us/step - loss: 0.0870 - mean_absolute_error: 0.1686 - val_loss: 0.0768 - val_mean_absolute_error: 0.1535\n",
      "Epoch 11/100\n",
      "637/637 [==============================] - 0s 119us/step - loss: 0.0822 - mean_absolute_error: 0.1697 - val_loss: 0.0721 - val_mean_absolute_error: 0.1535\n",
      "Epoch 12/100\n",
      "637/637 [==============================] - 0s 135us/step - loss: 0.0779 - mean_absolute_error: 0.1696 - val_loss: 0.0679 - val_mean_absolute_error: 0.1536\n",
      "Epoch 13/100\n",
      "637/637 [==============================] - 0s 152us/step - loss: 0.0736 - mean_absolute_error: 0.1685 - val_loss: 0.0641 - val_mean_absolute_error: 0.1533\n",
      "Epoch 14/100\n",
      "637/637 [==============================] - 0s 149us/step - loss: 0.0704 - mean_absolute_error: 0.1690 - val_loss: 0.0608 - val_mean_absolute_error: 0.1533\n",
      "Epoch 15/100\n",
      "637/637 [==============================] - 0s 141us/step - loss: 0.0667 - mean_absolute_error: 0.1679 - val_loss: 0.0578 - val_mean_absolute_error: 0.1530\n",
      "Epoch 16/100\n",
      "637/637 [==============================] - 0s 132us/step - loss: 0.0641 - mean_absolute_error: 0.1682 - val_loss: 0.0552 - val_mean_absolute_error: 0.1532\n",
      "Epoch 17/100\n",
      "637/637 [==============================] - 0s 135us/step - loss: 0.0616 - mean_absolute_error: 0.1681 - val_loss: 0.0528 - val_mean_absolute_error: 0.1531\n",
      "Epoch 18/100\n",
      "637/637 [==============================] - 0s 133us/step - loss: 0.0594 - mean_absolute_error: 0.1685 - val_loss: 0.0508 - val_mean_absolute_error: 0.1531\n",
      "Epoch 19/100\n",
      "637/637 [==============================] - 0s 126us/step - loss: 0.0573 - mean_absolute_error: 0.1680 - val_loss: 0.0489 - val_mean_absolute_error: 0.1530\n",
      "Epoch 20/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0556 - mean_absolute_error: 0.1678 - val_loss: 0.0472 - val_mean_absolute_error: 0.1529\n",
      "Epoch 21/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.0540 - mean_absolute_error: 0.1682 - val_loss: 0.0458 - val_mean_absolute_error: 0.1529\n",
      "Epoch 22/100\n",
      "637/637 [==============================] - 0s 119us/step - loss: 0.0528 - mean_absolute_error: 0.1679 - val_loss: 0.0445 - val_mean_absolute_error: 0.1528\n",
      "Epoch 23/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0515 - mean_absolute_error: 0.1679 - val_loss: 0.0433 - val_mean_absolute_error: 0.1529\n",
      "Epoch 24/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.0504 - mean_absolute_error: 0.1677 - val_loss: 0.0423 - val_mean_absolute_error: 0.1528\n",
      "Epoch 25/100\n",
      "637/637 [==============================] - 0s 141us/step - loss: 0.0495 - mean_absolute_error: 0.1684 - val_loss: 0.0414 - val_mean_absolute_error: 0.1528\n",
      "Epoch 26/100\n",
      "637/637 [==============================] - 0s 149us/step - loss: 0.0487 - mean_absolute_error: 0.1681 - val_loss: 0.0407 - val_mean_absolute_error: 0.1528\n",
      "Epoch 27/100\n",
      "637/637 [==============================] - 0s 149us/step - loss: 0.0479 - mean_absolute_error: 0.1680 - val_loss: 0.0399 - val_mean_absolute_error: 0.1528\n",
      "Epoch 28/100\n",
      "637/637 [==============================] - 0s 137us/step - loss: 0.0471 - mean_absolute_error: 0.1679 - val_loss: 0.0393 - val_mean_absolute_error: 0.1527\n",
      "Epoch 29/100\n",
      "637/637 [==============================] - 0s 130us/step - loss: 0.0466 - mean_absolute_error: 0.1678 - val_loss: 0.0388 - val_mean_absolute_error: 0.1527\n",
      "Epoch 30/100\n",
      "637/637 [==============================] - 0s 130us/step - loss: 0.0462 - mean_absolute_error: 0.1681 - val_loss: 0.0383 - val_mean_absolute_error: 0.1527\n",
      "Epoch 31/100\n",
      "637/637 [==============================] - 0s 124us/step - loss: 0.0458 - mean_absolute_error: 0.1682 - val_loss: 0.0379 - val_mean_absolute_error: 0.1527\n",
      "Epoch 32/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0454 - mean_absolute_error: 0.1678 - val_loss: 0.0375 - val_mean_absolute_error: 0.1527\n",
      "Epoch 33/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0450 - mean_absolute_error: 0.1678 - val_loss: 0.0372 - val_mean_absolute_error: 0.1527\n",
      "Epoch 34/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0448 - mean_absolute_error: 0.1682 - val_loss: 0.0369 - val_mean_absolute_error: 0.1527\n",
      "Epoch 35/100\n",
      "637/637 [==============================] - 0s 119us/step - loss: 0.0445 - mean_absolute_error: 0.1683 - val_loss: 0.0366 - val_mean_absolute_error: 0.1527\n",
      "Epoch 36/100\n",
      "637/637 [==============================] - 0s 119us/step - loss: 0.0441 - mean_absolute_error: 0.1679 - val_loss: 0.0364 - val_mean_absolute_error: 0.1526\n",
      "Epoch 37/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0438 - mean_absolute_error: 0.1678 - val_loss: 0.0362 - val_mean_absolute_error: 0.1526\n",
      "Epoch 38/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0436 - mean_absolute_error: 0.1677 - val_loss: 0.0360 - val_mean_absolute_error: 0.1527\n",
      "Epoch 39/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0435 - mean_absolute_error: 0.1679 - val_loss: 0.0358 - val_mean_absolute_error: 0.1526\n",
      "Epoch 40/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0434 - mean_absolute_error: 0.1678 - val_loss: 0.0357 - val_mean_absolute_error: 0.1526\n",
      "Epoch 41/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0433 - mean_absolute_error: 0.1680 - val_loss: 0.0355 - val_mean_absolute_error: 0.1526\n",
      "Epoch 42/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0432 - mean_absolute_error: 0.1678 - val_loss: 0.0354 - val_mean_absolute_error: 0.1526\n",
      "Epoch 43/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0430 - mean_absolute_error: 0.1677 - val_loss: 0.0353 - val_mean_absolute_error: 0.1525\n",
      "Epoch 44/100\n",
      "637/637 [==============================] - 0s 124us/step - loss: 0.0429 - mean_absolute_error: 0.1677 - val_loss: 0.0352 - val_mean_absolute_error: 0.1525\n",
      "Epoch 45/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0428 - mean_absolute_error: 0.1676 - val_loss: 0.0351 - val_mean_absolute_error: 0.1525\n",
      "Epoch 46/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0428 - mean_absolute_error: 0.1678 - val_loss: 0.0351 - val_mean_absolute_error: 0.1525\n",
      "Epoch 47/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0427 - mean_absolute_error: 0.1676 - val_loss: 0.0350 - val_mean_absolute_error: 0.1525\n",
      "Epoch 48/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0426 - mean_absolute_error: 0.1676 - val_loss: 0.0349 - val_mean_absolute_error: 0.1524\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637/637 [==============================] - 0s 115us/step - loss: 0.0426 - mean_absolute_error: 0.1676 - val_loss: 0.0349 - val_mean_absolute_error: 0.1524\n",
      "Epoch 50/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0425 - mean_absolute_error: 0.1676 - val_loss: 0.0348 - val_mean_absolute_error: 0.1524\n",
      "Epoch 51/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0425 - mean_absolute_error: 0.1677 - val_loss: 0.0348 - val_mean_absolute_error: 0.1524\n",
      "Epoch 52/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0424 - mean_absolute_error: 0.1676 - val_loss: 0.0348 - val_mean_absolute_error: 0.1524\n",
      "Epoch 53/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0424 - mean_absolute_error: 0.1677 - val_loss: 0.0347 - val_mean_absolute_error: 0.1524\n",
      "Epoch 54/100\n",
      "637/637 [==============================] - 0s 113us/step - loss: 0.0424 - mean_absolute_error: 0.1676 - val_loss: 0.0347 - val_mean_absolute_error: 0.1524\n",
      "Epoch 55/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0424 - mean_absolute_error: 0.1677 - val_loss: 0.0347 - val_mean_absolute_error: 0.1524\n",
      "Epoch 56/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0423 - mean_absolute_error: 0.1676 - val_loss: 0.0346 - val_mean_absolute_error: 0.1523\n",
      "Epoch 57/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.0423 - mean_absolute_error: 0.1676 - val_loss: 0.0346 - val_mean_absolute_error: 0.1524\n",
      "Epoch 58/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0423 - mean_absolute_error: 0.1676 - val_loss: 0.0346 - val_mean_absolute_error: 0.1524\n",
      "Epoch 59/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0422 - mean_absolute_error: 0.1675 - val_loss: 0.0346 - val_mean_absolute_error: 0.1523\n",
      "Epoch 60/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0422 - mean_absolute_error: 0.1675 - val_loss: 0.0346 - val_mean_absolute_error: 0.1524\n",
      "Epoch 61/100\n",
      "637/637 [==============================] - 0s 119us/step - loss: 0.0422 - mean_absolute_error: 0.1676 - val_loss: 0.0346 - val_mean_absolute_error: 0.1524\n",
      "Epoch 62/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0422 - mean_absolute_error: 0.1675 - val_loss: 0.0345 - val_mean_absolute_error: 0.1523\n",
      "Epoch 63/100\n",
      "637/637 [==============================] - 0s 121us/step - loss: 0.0422 - mean_absolute_error: 0.1676 - val_loss: 0.0345 - val_mean_absolute_error: 0.1524\n",
      "Epoch 64/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.0422 - mean_absolute_error: 0.1675 - val_loss: 0.0345 - val_mean_absolute_error: 0.1523\n",
      "Epoch 65/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0422 - mean_absolute_error: 0.1676 - val_loss: 0.0345 - val_mean_absolute_error: 0.1523\n",
      "Epoch 66/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0422 - mean_absolute_error: 0.1676 - val_loss: 0.0345 - val_mean_absolute_error: 0.1523\n",
      "Epoch 67/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0345 - val_mean_absolute_error: 0.1523\n",
      "Epoch 68/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0345 - val_mean_absolute_error: 0.1523\n",
      "Epoch 69/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0345 - val_mean_absolute_error: 0.1523\n",
      "Epoch 70/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0422 - mean_absolute_error: 0.1675 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 71/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1523\n",
      "Epoch 72/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 73/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 74/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 75/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 76/100\n",
      "637/637 [==============================] - 0s 119us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 77/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 78/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 79/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 80/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 81/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 82/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 83/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 84/100\n",
      "637/637 [==============================] - 0s 118us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 85/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 86/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 87/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 88/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 89/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 90/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 91/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1521\n",
      "Epoch 92/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 93/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 94/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 95/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 96/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 97/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1675 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 99/100\n",
      "637/637 [==============================] - 0s 115us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1522\n",
      "Epoch 100/100\n",
      "637/637 [==============================] - 0s 116us/step - loss: 0.0421 - mean_absolute_error: 0.1674 - val_loss: 0.0344 - val_mean_absolute_error: 0.1521\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from keras import regularizers\n",
    "model = Sequential([\n",
    "    Dense(2048, activation='tanh', kernel_initializer='uniform',kernel_regularizer=regularizers.l2(0.01), input_shape=(23,)),\n",
    "    Dropout(0.2),\n",
    "#    Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(1024, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(512, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(128, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "#    Dropout(0.2),\n",
    "#    Dense(32, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "#    Dropout(0.2),\n",
    "    Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(8, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))\n",
    "])\n",
    "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='hinge')\n",
    "#model.compile(optimizer='rmsprop', loss='hinge')\n",
    "\n",
    "#model.compile(optimizer='Adadelta', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "#model.compile(optimizer='sgd', loss='hinge', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "#, metrics=['accuracy'])\n",
    "#model.compile(optimizer='adam', loss='mean_squared_logarithmic_error')\n",
    "# model.compile(optimizer='rmsprop', loss=\"mean_absolute_percentage_error\")\n",
    "hist = model.fit(X_train, Y_train,\n",
    "          batch_size=32, epochs=100, shuffle=True,\n",
    "          validation_data=(X_val, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(hist.history['loss'])\n",
    "plt.plot(hist.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.56890285]] [[3.31]] [[9.8091545]]\n",
      "[[0.56890285]] [[4.51]] [[9.8091545]]\n",
      "[[0.56890285]] [[2.]] [[9.8091545]]\n",
      "[[0.56890285]] [[10.]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.]] [[9.8091545]]\n",
      "[[0.56890285]] [[8.43]] [[9.8091545]]\n",
      "[[0.56890285]] [[15.]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.69]] [[9.8091545]]\n",
      "[[0.56890285]] [[11.69]] [[9.8091545]]\n",
      "[[0.56890285]] [[15.]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.25]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.]] [[9.8091545]]\n",
      "[[0.56890285]] [[8.14]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.31]] [[9.8091545]]\n",
      "[[0.56890285]] [[11.16]] [[9.8091545]]\n",
      "[[0.56890285]] [[11.97]] [[9.8091545]]\n",
      "[[0.56890285]] [[11.43]] [[9.8091545]]\n",
      "[[0.56890285]] [[14.]] [[9.8091545]]\n",
      "[[0.56890285]] [[8.49]] [[9.8091545]]\n",
      "[[0.56890285]] [[10.11]] [[9.8091545]]\n",
      "[[0.56890285]] [[8.]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.85]] [[9.8091545]]\n",
      "[[0.56890285]] [[2.37]] [[9.8091545]]\n",
      "[[0.56890285]] [[7.]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.]] [[9.8091545]]\n",
      "[[0.56890285]] [[3.66]] [[9.8091545]]\n",
      "[[0.56890285]] [[7.27]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.05]] [[9.8091545]]\n",
      "[[0.56890285]] [[17.12]] [[9.8091545]]\n",
      "[[0.56890285]] [[14.]] [[9.8091545]]\n",
      "[[0.56890285]] [[11.58]] [[9.8091545]]\n",
      "[[0.56890285]] [[7.76]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.88]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.47]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.73]] [[9.8091545]]\n",
      "[[0.56890285]] [[7.1]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.23]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.]] [[9.8091545]]\n",
      "[[0.56890285]] [[7.49]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.61]] [[9.8091545]]\n",
      "[[0.56890285]] [[11.56]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.24]] [[9.8091545]]\n",
      "[[0.56890285]] [[16.95]] [[9.8091545]]\n",
      "[[0.56890285]] [[14.]] [[9.8091545]]\n",
      "[[0.56890285]] [[14.9]] [[9.8091545]]\n",
      "[[0.56890285]] [[11.61]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.46]] [[9.8091545]]\n",
      "[[0.56890285]] [[8.91]] [[9.8091545]]\n",
      "[[0.56890285]] [[10.]] [[9.8091545]]\n",
      "[[0.56890285]] [[5.58]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.71]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.]] [[9.8091545]]\n",
      "[[0.56890285]] [[8.2]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.12]] [[9.8091545]]\n",
      "[[0.56890285]] [[10.99]] [[9.8091545]]\n",
      "[[0.56890285]] [[2.]] [[9.8091545]]\n",
      "[[0.56890285]] [[15.05]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.]] [[9.8091545]]\n",
      "[[0.56890285]] [[5.6]] [[9.8091545]]\n",
      "[[0.56890285]] [[4.98]] [[9.8091545]]\n",
      "[[0.56890285]] [[11.41]] [[9.8091545]]\n",
      "[[0.56890285]] [[7.53]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.67]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.69]] [[9.8091545]]\n",
      "[[0.56890285]] [[8.54]] [[9.8091545]]\n",
      "[[0.56890285]] [[11.]] [[9.8091545]]\n",
      "[[0.56890285]] [[8.5]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.]] [[9.8091545]]\n",
      "[[0.56890285]] [[5.79]] [[9.8091545]]\n",
      "[[0.56890285]] [[10.05]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.]] [[9.8091545]]\n",
      "[[0.56890285]] [[2.39]] [[9.8091545]]\n",
      "[[0.56890285]] [[7.]] [[9.8091545]]\n",
      "[[0.56890285]] [[14.41]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.07]] [[9.8091545]]\n",
      "[[0.56890285]] [[2.55]] [[9.8091545]]\n",
      "[[0.56890285]] [[7.95]] [[9.8091545]]\n",
      "[[0.56890285]] [[0.]] [[9.8091545]]\n",
      "[[0.56890285]] [[1.]] [[9.8091545]]\n",
      "[[0.56890285]] [[8.1]] [[9.8091545]]\n",
      "[[0.56890285]] [[8.87]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.1]] [[9.8091545]]\n",
      "[[0.56890285]] [[9.]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.87]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.67]] [[9.8091545]]\n",
      "[[0.56890285]] [[14.]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.61]] [[9.8091545]]\n",
      "[[0.56890285]] [[3.23]] [[9.8091545]]\n",
      "[[0.56890285]] [[6.04]] [[9.8091545]]\n",
      "[[0.56890285]] [[3.03]] [[9.8091545]]\n",
      "[[0.56890285]] [[13.43]] [[9.8091545]]\n",
      "[[0.56890285]] [[14.27]] [[9.8091545]]\n",
      "[[0.56890285]] [[12.11]] [[9.8091545]]\n",
      "[[0.56890285]] [[10.69]] [[9.8091545]]\n",
      "[[0.56890285]] [[6.64]] [[9.8091545]]\n",
      "[[0.56890285]] [[7.]] [[9.8091545]]\n",
      "[[0.56890285]] [[10.49]] [[9.8091545]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(100):\n",
    "#    pass\n",
    "#    p = model.predict_on_batch(X_test[i].reshape(1, 23))\n",
    "    p = model.predict(X_test[i].reshape(1,23))\n",
    "    print(p,Y_scaler.inverse_transform(np.array(Y_test[i]).reshape(-1,1)), Y_scaler.inverse_transform(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637/637 [==============================] - 0s 50us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.042089703944592126, 0.16735904459114914]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.56890285]], dtype=float32),\n",
       " array([0.66315789]),\n",
       " array([[9.8091545]], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train[0].reshape(1, 23)),Y_train[0], Y_scaler.inverse_transform(model.predict(X_train[0].reshape(1, 23)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137/137 [==============================] - 0s 58us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03438613933585856, 0.1521286930698548]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2048)              49152     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                32784     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 82,081\n",
      "Trainable params: 82,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.56890285]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "hist.model.predict_on_batch(X_train[0].reshape(1,23))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
